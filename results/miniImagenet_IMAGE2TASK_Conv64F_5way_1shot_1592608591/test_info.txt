{'arch': {'atten_scale_value': 50, 'base_model': 'Conv64F', 'base_model_info': {}, 'from_value': 0.3, 'inplanes': 64, 'scale_value': 30, 'transfer_name': 'W', 'value_interval': 0.5}, 'data_name': 'miniImagenet', 'general': {'data_root': '/data/dcq/DataSets/miniImageNet', 'image2level': 'image2task', 'image_size': 84, 'print_freq': 100, 'query_num': 15, 'save_freq': 5, 'save_root': './results/', 'shot_num': 1, 'way_num': 5, 'workers_num': 8}, 'n_gpu': 1, 'test': {'batch_size': 1, 'episode_num': 1000}, 'train': {'batch_size': 1, 'episode_num': 5000, 'epochs': 50, 'loss': {'args': {}, 'name': 'CrossEntropyLoss'}, 'lr_scheduler': {'args': {'gamma': 0.5, 'milestones': [10, 15, 20, 25, 30, 35, 40, 45]}, 'name': 'MultiStepLR'}, 'optim_lr': 0.001}}
ALTNet(
  (features): CNNEncoder(
    (layer1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (layer2): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (layer3): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (layer4): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
    )
  )
  (metric_layer): ATLModule(
    (W): Sequential(
      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (attention_layer): AEAModule(
      (f_psi): Sequential(
        (0): Linear(in_features=64, out_features=4, bias=True)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Linear(in_features=4, out_features=1, bias=True)
        (3): Sigmoid()
      )
    )
  )
)
The num of the test_dataset: 600
============ Testing on the test set ============
Test-(0): [100/600]	Time 0.074 (0.098)	Loss 0.791 (1.134)	Prec@1 73.333 (55.630)
Test-(0): [200/600]	Time 0.018 (0.093)	Loss 0.952 (1.146)	Prec@1 61.333 (54.945)
Test-(0): [300/600]	Time 0.032 (0.091)	Loss 0.984 (1.159)	Prec@1 61.333 (54.383)
Test-(0): [400/600]	Time 0.017 (0.088)	Loss 1.326 (1.159)	Prec@1 42.667 (54.218)
Test-(0): [500/600]	Time 0.155 (0.088)	Loss 1.453 (1.153)	Prec@1 44.000 (54.515)
Test Accuracy: 54.0444450378418	 h: 0.7684504985809326
The num of the test_dataset: 600
============ Testing on the test set ============
Test-(1): [100/600]	Time 0.017 (0.100)	Loss 1.371 (1.165)	Prec@1 50.667 (54.257)
Test-(1): [200/600]	Time 0.088 (0.094)	Loss 0.967 (1.164)	Prec@1 65.333 (54.401)
Test-(1): [300/600]	Time 0.016 (0.092)	Loss 1.343 (1.149)	Prec@1 48.000 (54.928)
Test-(1): [400/600]	Time 0.104 (0.090)	Loss 1.618 (1.148)	Prec@1 41.333 (54.833)
Test-(1): [500/600]	Time 0.086 (0.090)	Loss 1.068 (1.155)	Prec@1 56.000 (54.582)
Test Accuracy: 54.704444885253906	 h: 0.8071883320808411
The num of the test_dataset: 600
============ Testing on the test set ============
Test-(2): [100/600]	Time 0.025 (0.097)	Loss 1.457 (1.151)	Prec@1 48.000 (54.073)
Test-(2): [200/600]	Time 0.102 (0.091)	Loss 1.146 (1.172)	Prec@1 56.000 (53.101)
Test-(2): [300/600]	Time 0.129 (0.090)	Loss 1.049 (1.160)	Prec@1 56.000 (53.617)
Test-(2): [400/600]	Time 0.017 (0.090)	Loss 0.951 (1.157)	Prec@1 64.000 (54.005)
Test-(2): [500/600]	Time 0.248 (0.090)	Loss 0.915 (1.157)	Prec@1 62.667 (54.097)
Test Accuracy: 54.13999938964844	 h: 0.7858568429946899
The num of the test_dataset: 600
============ Testing on the test set ============
Test-(3): [100/600]	Time 0.078 (0.105)	Loss 1.056 (1.165)	Prec@1 52.000 (54.680)
Test-(3): [200/600]	Time 0.017 (0.100)	Loss 1.350 (1.143)	Prec@1 42.667 (55.051)
Test-(3): [300/600]	Time 0.017 (0.102)	Loss 1.165 (1.142)	Prec@1 53.333 (55.092)
Test-(3): [400/600]	Time 0.020 (0.101)	Loss 1.175 (1.150)	Prec@1 53.333 (54.414)
Test-(3): [500/600]	Time 0.092 (0.101)	Loss 0.945 (1.151)	Prec@1 69.333 (54.358)
Test Accuracy: 54.21333312988281	 h: 0.7960272431373596
The num of the test_dataset: 600
============ Testing on the test set ============
Test-(4): [100/600]	Time 0.017 (0.109)	Loss 1.557 (1.161)	Prec@1 42.667 (54.007)
Test-(4): [200/600]	Time 0.057 (0.105)	Loss 0.982 (1.148)	Prec@1 60.000 (54.408)
Test-(4): [300/600]	Time 0.031 (0.104)	Loss 0.976 (1.151)	Prec@1 64.000 (54.321)
Test-(4): [400/600]	Time 0.120 (0.106)	Loss 1.147 (1.151)	Prec@1 57.333 (54.633)
Test-(4): [500/600]	Time 0.017 (0.105)	Loss 1.145 (1.148)	Prec@1 46.667 (54.675)
Test Accuracy: 54.52444839477539	 h: 0.7880120277404785
Aver Accuracy: 54.325	 Aver h: 0.789
............Testing is end............
